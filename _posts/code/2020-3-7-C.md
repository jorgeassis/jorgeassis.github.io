---
layout: post
lang: en
permalink: /download-biodiversity-records-iNaturalist/
comments: true

title: "Downloading biodiversity records from iNaturalist"

type: "code"
language: "python"
featured: "true"

description: "Automatically download biodiversity records from iNaturalist, the most recognised citizen science initiative."

citation: ""
link: ""

mainImage: unamed-unsplash.webp

date: 2021-03-23T10:00:00Z

---

iNaturalist is a global platform where naturalists, citizen scientists, and biologists post their observations with photographs. Observations can be curated by the network of users to provide valuable open data to scientific research projects, conservation agencies and the general public. In particular, the data has been used to describe the global distribution of species, address niche-based questions, support biodiversity and ecosystem-based conservation, and to understand correlations between anthropogenic pressures and population extinctions.

Data may be accessed via its website, mobile application or API. Here I provide some line of python code to automatically download multiple records from iNaturalist using the API. In this particular case, the code will get all data for the Parque Marinho Luiz Saldanha, a Marine Protected Area located in Portugal.

<div style="padding: 20px" class="border-radius-05 bg-gray font-family-secondary font-small text-dark">

# import dependencies<br>
from requests import request<br>
import json<br>
from pandas.io.json import json_normalize<br>
import pandas as pd<br>
import urllib.request<br>
import os<br><br>

# Get all records for Parque Marinho Luiz Saldanha<br>
outputFolder = '/theFolderToSaveData'<br>
placeID = '128892'<br>
if not os.path.exists(outputFolder):<br>
    os.makedirs(outputFolder)<br>
df = pd.DataFrame([])<br>
iteraction = 0<br>
dataSize = 1<br>
while (dataSize > 0):<br>
    iteraction = iteraction + 1<br>
    url = 'https://api.inaturalist.org/v1/<br>
    observations?geo=true&identified=true&place_id='+placeID+'&rank=species&<br>
    order=desc&order_by=created_at&page='+str(iteraction)+'&per_page=200'<br>
    response = requests.get(url)<br>
    dictr = response.json()<br>
    recs = dictr['results']<br>
    dataSize = len(recs)<br>
    data = json_normalize(recs)<br>
    if dataSize > 0:<br>
        df = df.append(data)<br><br>

# Save images to the folder<br>
for x in range(0, len(df)):<br>
    id = df['id'].values[x]<br>
    imageURL = json_normalize(df['photos'].values[x])['url'].values[0]<br>
    imageURLM = imageURL.replace("square", "medium")<br>
    imageURLO = imageURL.replace("square", "original")<br>
    urllib.request.urlretrieve(imageURL, outputFolder+'/'+str(id)+'Sq.jpg')<br>
    urllib.request.urlretrieve(imageURLM, outputFolder+'/'+str(id)+'M.jpg')<br>
    urllib.request.urlretrieve(imageURLO, outputFolder+'/'+str(id)+'O.jpg')<br><br>

# Prepare pandas data.frame and export to csv<br>
list(df.columns)<br>
dfLatitude = df["location"].str.split(",", expand = True)[0]<br>
dfLongitude= df["location"].str.split(",", expand = True)[1]<br>
df1 = df[['id','taxon.name', 'quality_grade', 'time_observed_at', 'species_guess', 'license_code', 'observed_on', <br> 'community_taxon_id']]<br>
df1 = pd.concat([df1, dfLongitude, dfLatitude], axis=1)<br>
df1.columns = ['id','taxon.name', 'quality_grade', 'time_observed_at', 'species_guess', 'license_code', 'observed_on', <br> 'community_taxon_id','Longitude','Latitude']<br>
df1.to_csv(outputFolder+'/dataFrame.csv', sep='\t', header=True)
</div>
